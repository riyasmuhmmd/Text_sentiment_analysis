# -*- coding: utf-8 -*-
"""NLP_in_pytorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MfhmxSX8e0s-DO1m_d0uj6Uz3nhdCoB-
"""

import pandas as pd
import numpy as np
import re


import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer

import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm

nltk.download('all')

from dataclasses import dataclass
import os
import logging
import pickle

class DataIngestionConfig:
    raw_data_path: str = os.path.join("artifact", "raw.csv")
    train_data_path: str = os.path.join("artifact", "train.csv")
    test_data_path: str = os.path.join("artifact", "test.csv")


class DataTransformationConfig:
    preprocesser_path: str = os.path.join("artifact", "text_clean_preprocessor.pkl")
    vectorizer_path: str = os.path.join("artifact", "vectorizer.pkl")
    le_encoder_path: str = os.path.join("artifact", "le_encoder.pkl")


class ModelTrainerConfig:
    model_path: str = os.path.join("artifact", "model.pkl")

def save_object(file_path, obj):
    try:
        dir_path = os.path.dirname(file_path)

        os.makedirs(dir_path, exist_ok=True)

        with open(file_path, "wb") as file_obj:
            pickle.dump(obj, file_obj)

    except Exception as e:
        raise ValueError(e)


def load_object(file_path):
    try:
        with open(file_path,'rb') as file_obj:
            return pickle.load(file_obj)
    except Exception as e:
        logging.info('Exception Occured in load_object function utils')
        raise ValueError(e)

class DataIngestion:
    def __init__(self):

        try:
            self.data_ingestion_config = DataIngestionConfig()
        except Exception as e:
            raise ValueError(e)

    def download_Data(self):

        try:
            df = pd.read_csv('Twitter_Data.csv')

            df.reset_index(drop=True, inplace=True)

            df.drop(columns=df.columns[[0,2]], axis=1, inplace=True)
            df.columns = ['text','sentiment']
            df.drop([0], axis=0, inplace=True)
            #df.sentiment = df.sentiment.map({"neutral":0,"positive":1,"negative":2})
            df.dropna(inplace=True)

            return df

        except Exception as e:
            raise ValueError(e)

    def intiate_data_ingestion(self):

        try:
            df = self.download_Data()

            logging.info("Data download ..............")

            os.makedirs(os.path.dirname(self.data_ingestion_config.raw_data_path), exist_ok=True)

            df.to_csv(self.data_ingestion_config.raw_data_path, index=False)

            train, test = train_test_split(df, test_size=0.2)
            train.to_csv(self.data_ingestion_config.train_data_path, index=False)
            test.to_csv(self.data_ingestion_config.test_data_path, index=False)
            logging.info("Data Ingestion Completed ..............")
            return (
                self.data_ingestion_config.train_data_path,
                self.data_ingestion_config.test_data_path
            )
        except Exception as e:
            raise ValueError(e)

class DataTratanformation:

    def __init__(self):
        try:
            self.data_transformation_config = DataTransformationConfig()
        except Exception as e:
            raise ValueError(e)

    def preprocess(self, df):
        # Apply the preprocessing function to the 'text' column of the DataFrame
        df['text'] = df['text'].astype(str).apply(self._preprocess_text)  # Convert column to string type and apply the helper function
        return df

    def _preprocess_text(self, text):  # Helper function to process individual text strings
        text = re.sub(r"http\\S+", "", text)
        text = re.sub(r"@\\w+", "", text)
        text = re.sub(r"#", "", text)
        text = re.sub(r"[^\w\s]", "", text)
        text = str(text).lower()
        text = text.split()
        lemmatizer = WordNetLemmatizer()
        clean_text = [lemmatizer.lemmatize(word) for word in text if not word in set(stopwords.words('english'))]
        clean_text = ' '.join(clean_text)
        return clean_text

    def fit_transformation_tensor(self, train, test):

        try:
            train_data = pd.read_csv(train)
            test_data = pd.read_csv(test)

            # Preprocess before splitting
            train_data = self.preprocess(train_data)
            test_data = self.preprocess(test_data)

            save_object(
                self.data_transformation_config.preprocesser_path,
                self.preprocess
            )
            le = LabelEncoder()
            train_data['sentiment'] = le.fit_transform(train_data['sentiment'])
            test_data['sentiment'] = le.transform(test_data['sentiment'])
            print(train_data.sentiment.value_counts(normalize=True)*100)
            save_object(
                self.data_transformation_config.le_encoder_path,
                le
            )

            # Split the data after preprocessing
            X_train, X_test, y_train, y_test = train_test_split(train_data['text'], train_data['sentiment'], test_size=0.2)

            vectorizer = TfidfVectorizer(max_features=5000)
            X_train_vectorized = vectorizer.fit_transform(X_train)
            X_test_vectorized = vectorizer.transform(X_test)
            X_train_vectorized = X_train_vectorized.toarray()
            X_test_vectorized = X_test_vectorized.toarray()

            save_object(
                self.data_transformation_config.vectorizer_path,
                vectorizer
            )

            X_train_tensor = torch.tensor(X_train_vectorized, dtype=torch.float32)
            X_test_tensor = torch.tensor(X_test_vectorized, dtype=torch.float32)
            y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)
            y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)

            return (
                X_train_tensor,
                X_test_tensor,
                y_train_tensor,
                y_test_tensor
            )
        except Exception as e:
            raise ValueError(e)

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)
        self.fc1 = nn.Linear(hidden_size, hidden_size)
        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)
        self.fc2 = nn.Linear(hidden_size, num_classes)
    def forward(self, x):
        # Reshape input to (batch_size, seq_length, input_size)
        x = x.unsqueeze(1)

        # LSTM forward pass
        out, (hn, cn) = self.lstm(x)

        # Pass the last hidden state through the first linear layer
        out = self.fc1(hn[-1])

        # Apply Leaky ReLU activation
        out = self.leaky_relu(out)

        # Pass through the second linear layer to get final output
        out = self.fc2(out)

        return out

class ModelTrainer:
    def __init__(self, model, X_train, y_train, X_test, y_test):
        try:
            self.model_trainer_config = ModelTrainerConfig()
            self.model = model
            self.X_train = X_train
            self.y_train = y_train
            self.X_test = X_test
            self.y_test = y_test
        except Exception as e:
            raise ValueError(e)


    def fit_trainer(self):
        try:
            self._train_model()
            save_object(
                self.model_trainer_config.model_path,
                self.model
            )
        except Exception as e:
            raise ValueError(e)


    def fit_evaluation(self):

        try:
            self._model_evaluation()
        except Exception as e:
            raise ValueError(e)

    def _train_model(self):

        try:
            criterion = nn.CrossEntropyLoss()
            optimizer = optim.Adam(model.parameters(), lr=0.001)
            epochs = 5
            for epoch in range(epochs):
                self.model.train()

                running_loss = 0.0

                for i in tqdm(range(0, len(self.X_train), 64)):
                    X_batch = self.X_train[i:i + 64]
                    y_batch = self.y_train[i:i + 64]

                    optimizer.zero_grad()
                    outputs = model(X_batch)
                    loss = criterion(outputs, y_batch)
                    loss.backward()
                    accuracy = (outputs.argmax(dim=1) == y_batch).float().mean()
                    optimizer.step()
                    running_loss += loss.item()
                print(f'Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(self.X_train):.4f}, Accuracy: {accuracy:.4f}')

        except Exception as e:
            raise ValueError(e)

    def _model_evaluation(self):

        try:
            self.model.eval()

            with torch.no_grad():
                outputs = model(self.X_test)
                _, predicted = torch.max(outputs.data, 1)

            accuracy = (predicted == self.y_test).sum().item() / len(self.y_test) * 100
            print(f'Test Accuracy: {accuracy:.2f}%')

        except Exception as e:
            raise ValueError(e)

data = DataIngestion()
train_ptl_, test_ptl_ = data.intiate_data_ingestion() # Assign the paths

data_t = DataTratanformation()
xtrain_tensor, xtest_tensor, ytrain_tensor_, ytest_tensor_ = data_t.fit_transformation_tensor(train_ptl_, test_ptl_)

xtrain_tensor.shape, xtest_tensor.shape, ytrain_tensor_.shape, ytest_tensor_.shape

# Initialize model parameters
input_size = xtrain_tensor.shape[1]  # Input feature size
hidden_size = 64                        # Size of LSTM hidden state
num_classes = len(np.unique(ytrain_tensor_))   # Number of output classes

# Instantiate the model
model = LSTMModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes)

model_trainer = ModelTrainer(model, xtrain_tensor,
                             ytrain_tensor_,
                             xtest_tensor,
                             ytest_tensor_
                            )
model_trainer.fit_trainer()

model_trainer.fit_evaluation()

le = os.path.join("artifact", "le_encoder.pkl")
leb = load_object(le)
vectorizer = os.path.join("artifact", "vectorizer.pkl")
vectorizer = load_object(vectorizer)
model_path = os.path.join("artifact", "model.pkl")
model_1 = load_object(model_path)
text_path = os.path.join("artifact", "text_clean_preprocessor.pkl")
text_clean_preprocessor = load_object(text_path)

class PredictionPipeline:
    def __init__(self):

        pass

    @staticmethod
    def preprocess_text(text):
        text = re.sub(r"http\\S+", "", text)
        text = re.sub(r"@\\w+", "", text)
        text = re.sub(r"#", "", text)
        text = re.sub(r"[^\w\s]", "", text)
        text = str(text).lower()
        text = text.split()
        lemmatizer = WordNetLemmatizer()
        clean_text = [lemmatizer.lemmatize(word) for word in text if not word in set(stopwords.words('english'))]
        clean_text = ' '.join(clean_text)
        return clean_text

    def predict(self, text):

        try:
            le = os.path.join("artifact", "le_encoder.pkl")
            leb = load_object(le)

            vectorizer = os.path.join("artifact", "vectorizer.pkl")
            vectorizer = load_object(vectorizer)

            model_path = os.path.join("artifact", "model.pkl")
            model_1 = load_object(model_path)

            text_clean = PredictionPipeline.preprocess_text(text)
            text_vectorized = vectorizer.transform([text_clean]).toarray()
            text_tensor = torch.FloatTensor(text_vectorized)
            model_1.eval()
            with torch.no_grad():
                output = model_1(text_tensor)
                predicted = torch.argmax(output.data, 1)

            sentiment_label = leb.inverse_transform(predicted.cpu().numpy())
            return print('Predicted Sentiment:',sentiment_label[0])

        except Exception as e:
            raise ValueError(e)

text = 'there are people and then there are pencils some are sharp, some are not and some can be sharpened  my pencil philosophy.....'
PredictionPipeline().predict(text)